{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyN0SD9Nf71c1FiJtD0Xlkbu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/LING-226-vuw/blob/main/04_Types_Tokens_TTR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcNVXxLWYcDl"
      },
      "source": [
        "# **What is a word?**\n",
        "\n",
        "This might seem like a silly question, but it's a crucual question to ask when we want to think about the computational representation of text and language. We have seen how text can be stored as a single string in Python, and there are no real limits to how long this string can be. But a string is still seen as a single sequence of characters, and thus the most interesting information we can obtain from a string is how long a string is in terms of number of characters.\n",
        "\n",
        "What if we wanted to know how many words are in a string? What about sentences? Paragraphs?\n",
        "\n",
        "We need a method to *split* or *segment* the string so that we can extract the individual words. That means we need to create a set of rules or principles by which we can determine individual words in a string.\n",
        "\n",
        "Hence the question: what is a word? Or, perhaps phrased slightly differently, how do we distinguish words from one another when reading? The answer for written English and many other written forms of language is relatively simple - whitespace. The spaces between words represent simple yet effective boundaries to indicate where words begin and end. (Note that other languages, such as Chinese, do **not** use whitespace, which means many of the methods we use in this notebook require adaptation when applied to other languages).\n",
        "\n",
        "Let us proceed for the moment with the knowledge the segementing words on whitespace could be a helpful way to split a string into words. And, it just so happens that Python has a built-in function which allows us to do this with ease: the `string.split()` function.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using `.split()` to create words from strings.\n",
        "\n",
        " `.split()` is string method which means it is specific to string types. By default, `.split()` will search a string for any whitespace character and then split the string on those whitespaces. The whitespaces will be effectively removed from the string, and the resulting chunks will be placed into a list, with each segment representing a value in the list. Consider the example below:"
      ],
      "metadata": {
        "id": "G6vt3MT9CfmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a string and save it to a variable\n",
        "pretzels = 'these pretzels are making me thirsty'\n",
        "\n",
        "# use .split() to convert the string into a list of segments split on whitespace\n",
        "pretzels.split()"
      ],
      "metadata": {
        "id": "r9f9TcuSCtFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pretty neat, right? What's more is that `.split()` can be used to split a string on **any** character one likes - the default is whitespace but you could choose anything, such as a certain character or letter:"
      ],
      "metadata": {
        "id": "PHiE9x8pDVfR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# split at \"o\", note that the \"o\" is not included in the output (just like whitespace)\n",
        "'Melodrama'.split('o')"
      ],
      "metadata": {
        "id": "bocVFycaDh5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can you figure out how to use `.split()` as a means to count the total number of words in a string? Hint: do you remember the `len()` function? Use `.split()` to count the number of words in the following sentence:"
      ],
      "metadata": {
        "id": "a7_P7c7UD1hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# can you use .split() and .len() to find the total number of words in this sentence?\n",
        "# the \"\\\" is used to 'escape' the quote so that python doesn't read it as a delimiter\n",
        "example = 'what if everything around you isn\\'t quite as it seems?'\n"
      ],
      "metadata": {
        "id": "hN09E31VD_Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# the answer...\n",
        "len(example.split())"
      ],
      "metadata": {
        "id": "F3wclykJD7LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we don't actually need to save the string to a variable first - we could wrap `len()` around the string, we just have to keep our brackets in the right place!"
      ],
      "metadata": {
        "id": "j_VvUHm-6Ql0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len('what if everything around you isn\\'t quite as it seems?'.split())"
      ],
      "metadata": {
        "id": "1YNTV8A46bdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Types and Tokens\n",
        "\n",
        "Now that we know how to split a string into different words, we can start using Python to count the number of words in a string. Not only can we find the total number of words in a string, we can also find the total number of *unique* words. The distinction between unique words and total words has specific terminology:\n",
        "\n",
        "- A ***type*** is a unique word.\n",
        "- A ***token*** is an individual occurence of a type.\n",
        "\n",
        "For example - pretend you had three pets at home: two dogs and a cat. If we sorted our pets into types and tokens, we would have three tokens (three pets total), but only two types: dog and cat.\n",
        "\n",
        "In Section 1.4 of Chapter 1, the NLKT book asks you to think about types and tokens by introducing you to the `set()` and `len()` functions. You have already used `len()`, but `set()` might be new. Using `set()` returns a data container that only allows one of any value to exist in the container. In other words, it returns an object where repeated values are not allowed.\n",
        "\n",
        "This means we can use `set()` to ask for unique values in our data.\n"
      ],
      "metadata": {
        "id": "QzApZUwVBpTs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For example, what does set() give you here?\n",
        "set('aaabbbccc')"
      ],
      "metadata": {
        "id": "LfqVneTaWbbA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# and here?\n",
        "set(['a', 'a', 'a', 'b', 'b', 'b', 'c','c', 'c'])"
      ],
      "metadata": {
        "id": "eH7TVL9uWoz7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's think about how to use `len()`and `set()` to compute the Types and Tokens for a string.\n",
        "\n",
        "- First we'll define a string\n",
        "- then we will split the string into a list\n",
        "- then we will measure the length of the list\n",
        "- then we will measure the length of unique items in the list\n",
        "\n",
        "We'll also chain some of these functions together and save them to new variables."
      ],
      "metadata": {
        "id": "NSH6NiXdexVl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9i7iS6CLZlKS"
      },
      "outputs": [],
      "source": [
        "# start with a string\n",
        "mood_raw = \"i can\\'t feel a thing, i keep looking at my mood ring\"\n",
        "\n",
        "# create a split version\n",
        "mood_tokens = mood_raw.split()\n",
        "\n",
        "# compare the before/after\n",
        "print(mood_raw, '\\n\\n', mood_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# measure the total number of words\n",
        "total_mood_tokens = len(mood_tokens)\n",
        "total_mood_tokens"
      ],
      "metadata": {
        "id": "mzRSPN3xgdb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# measure the number of types\n",
        "# how else could you write this code for the same effect?\n",
        "total_mood_types = len(set(mood_raw.split()))\n",
        "total_mood_types"
      ],
      "metadata": {
        "id": "QqJlyGGGgqx6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Your Turn**\n",
        "\n",
        "Before moving on, make sure you are comfortable with the difference between types and tokens, as well as how to use `len()` and `set()`."
      ],
      "metadata": {
        "id": "6U7Sx24koY2h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# take this moment to practice with set() and len()"
      ],
      "metadata": {
        "id": "2y_G3QuzojZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Measuring Lexical Diversity**\n",
        "We can now use this information to assess our text for a very basic measure of sophistication: lexical diversity. This is also known as a type-token ratio, and provides a measure of how many repeated words there are in a text. You can read more about it in [Chapter 1 of NLTK.](https://www.nltk.org/book/ch01.html)\n",
        "\n",
        "To calculate lexical diveristy, we can use the following formula:\n",
        "\n",
        "> `number of types / number of tokens`\n",
        "\n",
        "The code cell below creates a user-defined **function** named `lexical_diversity()` to calculate lexical diversity. Running the code cell below will load the function into memory, allowing you to use it just like other functions such as `print()`, `len()`, and `set()`.\n",
        "\n",
        "The name of the function is `lexical_diversity`, and it expects an input called `tokens`.\n",
        "\n",
        "The function will `return` (i.e., give back) the result of measuring the length of dividing the `set()` of tokens by the `len()` of tokens.\n",
        "\n",
        "Creating functions will be covered in more detail later on."
      ],
      "metadata": {
        "id": "lDyLumQool5Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function to calculate lexical diversity\n",
        "def lexical_diversity(tokens):\n",
        "  # return the result of dividing the length\n",
        "  return len(set(tokens))/len(tokens)"
      ],
      "metadata": {
        "id": "t3qz0s__uhGG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use the function, we just need to type its name and provide a list of tokens as the argument. This works just like `print()`, `len()`, and other Python functions we have been using.\n",
        "\n",
        "So, what is the lexical diversity or TTR of our example sentence?\n",
        "\n"
      ],
      "metadata": {
        "id": "TR4RvV8zhDmw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0cg160WMbHVq"
      },
      "outputs": [],
      "source": [
        "# what is the TTR of our example sentence?\n",
        "\n",
        "# first create a list of tokens using .split()\n",
        "mood_tokens = mood_raw.split()\n",
        "\n",
        "# then run the function\n",
        "lexical_diversity(mood_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We get a result of .916, in other words 91.6% of our tokens are represented by a single type, indicating a very high lexical diversity.\n",
        "\n",
        "Of course, such measures are relatively meaningless on such a short amount of text - the true use of lexical diversity would be to compare much larger texts against one another.\n",
        "\n",
        "Nonetheless, try the lexical diversity function on some examples yourself to see how repeating words influence the overall score.\n",
        "\n",
        "> ***Important!*** You need to feed a list of tokens to `lexical_diversity()`, otherwise you will get the diversity based on **characters** in the string, not words! What could you do to modify this?"
      ],
      "metadata": {
        "id": "_fQSLMtDv6-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexical diversity of 50%\n",
        "# two types, four tokens\n",
        "# 2/4 = .5 (50%)\n",
        "lexical_diversity('hello world hello world'.split())"
      ],
      "metadata": {
        "id": "xim13vzXwhSu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lexical diversity of 100%\n",
        "# two types, two tokens\n",
        "# 2/2 = 1 (100%)\n",
        "lexical_diversity('hello world'.split())"
      ],
      "metadata": {
        "id": "p8_JWEvf0NR0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Your Turn**\n",
        "\n",
        "Try out the lexical diversity function on some text. The function expects raw string as input."
      ],
      "metadata": {
        "id": "T9PgkjmnNBUY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Play with lexical diversity on your own examples\n"
      ],
      "metadata": {
        "id": "YxHKIMiS0kxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Comparing Lexical Diversity of Two Texts\n",
        "\n",
        "Now that we know how to calculate the lexical diversity of a single text, let's expand the function to compare the lexical diversity of two texts.\n",
        "\n",
        "We will also modify the function so that you can input a raw string to the function, rather than a list of tokens. Read the comments to see if you understand how the function works.\n",
        "\n",
        "Remember, we need to first load the function into memory before we can use it.\n"
      ],
      "metadata": {
        "id": "2KaKfIqWoERr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nxfSCL_je9TG"
      },
      "outputs": [],
      "source": [
        "# create a new function which takes two texts as arguments:\n",
        "def compare_lexical_diversity(text1, text2):\n",
        "\n",
        "  # in one line, caclulate LD of text1 using .split()\n",
        "  text_1_ld = len(set(text1.split()))/len(text1.split())\n",
        "  # repeat for the second text\n",
        "  text_2_ld = len(set(text2.split()))/len(text2.split())\n",
        "\n",
        "  # print out the results\n",
        "  print(f\"{text1}\\nLexical Diversity: {text_1_ld} \\n\\n{text2}\\nLexical Diversity: {text_2_ld}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### string formatting\n",
        "\n",
        "You probably see that in the `print()` line in the function looks a little crazy. It uses something called string formatting, where you can combine variables and text inside a single print statement.\n",
        "\n",
        "The `f` in front of the string is all you need to do to activate this format. Then, inside the string delimiters, you can use curly brackets `{}` to include variables and other Python functions.\n",
        "\n",
        "The `\\n` is a character which stands for newline, which has the same effect as pressing the enter key on your keyboard to create a new line in a text."
      ],
      "metadata": {
        "id": "koodqGNF9Mtp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, let's create two texts and analyze them using this new function. You'll see that I use triple quotes as a means to make it easier to encase the entire text on multiple lines. This is mainly for aesthetic reasons."
      ],
      "metadata": {
        "id": "b00-RKHE9shL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create two strings as our texts\n",
        "turtles = \"\"\"teenage mutant ninja turtles\n",
        "teenage mutant ninja turtles\n",
        "teenage mutant ninja turtles\n",
        "heroes in a halfshell turtle power!\"\"\"\n",
        "\n",
        "\n",
        "baby_shark = \"\"\"Baby Shark doo-doo doo-doo\n",
        "Baby Shark doo-doo doo-doo\n",
        "Baby Shark doo-doo doo-doo\n",
        "Baby Shark\"\"\"\n"
      ],
      "metadata": {
        "id": "gFucwwZBrRyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ok, time to run the function! But, before doing so, can you predict which song *should* be more lexically diverse? Remember, more repetitions of the same word means there will be *lower* lexical diversity. So, which song repeats more words?"
      ],
      "metadata": {
        "id": "NmkPLbCh-HxJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5dpZ0OKffK3R"
      },
      "outputs": [],
      "source": [
        "# which song is more lexically diverse?\n",
        "compare_lexical_diversity(turtles, baby_shark)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Your Turn**\n",
        "\n",
        "Spend some time comparing the lexical diversity of different strings."
      ],
      "metadata": {
        "id": "q2YPlcbAtDr_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UzD0OD17tJDn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Improving Lexical Diversity\n",
        "\n",
        "Lexical Diversity based on TTR is a very crude measure of vocabulary diversity. The above examples have used very short texts, and the resulting TTR values are somewhat meaningful. However, TTR has been heavily criticized for being a relatively poor measure of lexical diversity. Why is that? At least for English, one reason is that the overall length of a text will directly influence TTR. Longer texts will naturally start to repeat certain words again, such as function words like `the`, `a`, and `an`.\n",
        "\n",
        "To account for this problem, there are a surprisingly large number of different TTR metrics that have been developed. Many of them try to include some sort of average TTR by moving over and calculating TTR for portions of the text, or using some other sort of function which helps address the effects of text length. Some other research also points out that human measures of lexical diversity are not being captured in measures such as TTR.\n",
        "\n",
        "\n",
        "So, know that other measures exist. To wrap this notebook up, let's try to \"prove\" how text length might influence TTR while also getting some practice with function writing :)\n",
        "\n",
        "We are going to write a function which splits a text into segments and then calculates the TTR for each segment, and then averages those TTR values to get an average TTR.\n",
        "\n",
        "But first, let's consider how we might split a text into sentences. You might have noticed me using the `\\n` character in some of the print statements, this character represents a `newline` such as when you press the enter/return key to start a new paragraph.\n",
        "\n",
        "Remember that `.split()` will let you split a string on any character, this includes newlines! Consider the text below, it is all encoded as a single string, but because I copied and pasted it from a website in this format, it was retained new lines (I also use triple quotes \"\"\" to allow the string to span across these newlines). Run the cells to see how splitting on newlines can give us sentences:"
      ],
      "metadata": {
        "id": "4UengCkstNR6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's read in a longer string\n",
        "rwib = \"\"\"See the animal in his cage that you built.\n",
        "Are you sure what side you're on?\n",
        "Better not look him too closely in the eye.\n",
        "Are you sure what side of the glass you are on?\n",
        "See the safety of the life you have built.\n",
        "Everything where it belongs\n",
        "Feel the hollowness inside of your heart\n",
        "And it's all\n",
        "Right where it belongs\n",
        "What if everything around you\n",
        "Isn't quite as it seems?\n",
        "What if all the world you think you know\n",
        "Is an elaborate dream?\n",
        "And if you look at your reflection\n",
        "Is it all you want it to be?\n",
        "What if you could look right\n",
        "Through the cracks?\n",
        "Would you find yourself\n",
        "Find yourself afraid to see?\"\"\"\n"
      ],
      "metadata": {
        "id": "_yHvWmfjw6YK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the string on `\\n` means the resulting list will be approximately sentences, rather than single words. Cool!"
      ],
      "metadata": {
        "id": "nW9cYLMt_DlB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rwib.split('\\n')"
      ],
      "metadata": {
        "id": "gLoB6_L6xe8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that have figured out a way to split a string into sentences, we can build our average TTR function. I am going to include a `for loop` inside this function. We will cover these in more detail later on, but you might be able to understand the logic of it now. Basically, a `for loop` will repeat the same operation on each member of a specified population.  \n",
        "\n",
        "If you want to explore the function as it works, you can uncomment the print statements."
      ],
      "metadata": {
        "id": "loMLr53WyHs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define a function named average_ttr\n",
        "def average_ttr(text):\n",
        "\n",
        "  # create an empty list to store the TTR values\n",
        "  ttr_values = []\n",
        "\n",
        "  # split the raw string input into sentences using the newline character\n",
        "  # the result is a list with each value being a sentence\n",
        "  sentences = text.split('\\n')\n",
        "\n",
        "  # loop through each value/sentence of sentences, one at a time.\n",
        "  for sent in sentences:\n",
        "\n",
        "    # calclulate the TTR of the sentence\n",
        "    # print(sent)\n",
        "    sent_ld = len(set(sent.split()))/len(sent.split())\n",
        "\n",
        "    # add the value to the list using .append()\n",
        "    ttr_values.append(sent_ld)\n",
        "\n",
        "  # use sum() to add the TTR values together,\n",
        "  # then divide by total number of values (using len()) to get average TTR\n",
        "  # print(ttr_values)\n",
        "  average_ttr = sum(ttr_values)/len(ttr_values)\n",
        "\n",
        "  print(f\"Average Sentence TTR is: {average_ttr}\")\n"
      ],
      "metadata": {
        "id": "F4Wx_p00u4ba"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The averaget TTR is .97 or 97%, meaning it is a pretty diverse text!"
      ],
      "metadata": {
        "id": "xocwTEe-_x81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "average_ttr(rwib)"
      ],
      "metadata": {
        "id": "39WfWCSuyyKC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What happens if we calculate the lexical diversity of the entire song at once? The TTR is shockingly lower!"
      ],
      "metadata": {
        "id": "OnK6ZWmq_3oO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(set(rwib.split())) / len(rwib.split())"
      ],
      "metadata": {
        "id": "S3d5l-5F_ggQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Discussion\n",
        "\n",
        "Play around with the average TTR function and different texts.\n",
        "- You might want to explore splitting texts on different things besdies newlines such as periods\n",
        "- How does the average TTR function compare to the whole text TTR? Compare the results between the two for the same texts.\n",
        "- Does calculating average sentence TTR create any new problems with TTR?"
      ],
      "metadata": {
        "id": "cNVMIgpvz4zP"
      }
    }
  ]
}