{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scskalicky/VocabAtVic2023NLPWorkshop/blob/main/03-NLTK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BcNVXxLWYcDl"
      },
      "source": [
        "## **Strings into Words**\n",
        "\n",
        "- How can we get Python to turn a `string` into a set of words?\n",
        "- the `str.split()` function is one way\n",
        "- this function will \"split\" a string on a pre-defined character. The default is to split on whitespace:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9f9TcuSCtFS"
      },
      "outputs": [],
      "source": [
        "# define a string and save it to a variable\n",
        "pretzels = 'these pretzels are making me thirsty'\n",
        "\n",
        "# use .split() to convert the string into a list of segments split on whitespace\n",
        "pretzels.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The resulting object was a `list` of values, which happen to be individual words!\n",
        "\n",
        "Do you remember `len()`? How can we use `len()` and `str.split()` to count the number of words in a string?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# how to use len() and .split() to count the total number of words?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Types, tokens, and TTR**\n",
        "\n",
        "- types: unique words\n",
        "- tokens: occurences of unique words\n",
        "- type-token ratio (TTR): measure of lexical repetition in text\n",
        "\n",
        "#### use `set()` to find unique instances of items in a sequence\n",
        "- `set(tokens)` = all unique values\n",
        "\n",
        "#### use `len()` and `set()` to calculate TTR\n",
        "\n",
        "- `len(set(tokens))` / `len(tokens)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# load in a text\n",
        "turtles = \"\"\"teenage mutant ninja turtles, \n",
        "            teenage mutant ninja turtles, \n",
        "            teenage mutant ninja turtles, \n",
        "            heroes in a halfshell, turtle power!\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# convert to tokens using `.split()`\n",
        "turtle_tokens = turtles.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# look at the tokens\n",
        "turtle_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use `len()` to count the number of words\n",
        "len(turtle_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use `set()` to get just the unique words\n",
        "turtle_types = set(turtle_tokens)\n",
        "turtle_types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use len() and set() to count the number of types..\n",
        "len(turtle_types)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# finally...calculate the TTR!\n",
        "len(turtle_types) / len(turtle_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Problems with `.split()`\n",
        "\n",
        "- punctuation is retained\n",
        "- not all languages use whitespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "yadda = 'yadda, yadda, yadda!'\n",
        "\n",
        "set(yadda.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# not all languages use whitespace\n",
        "zhongwen = '对不起我的中文不好'\n",
        "\n",
        "zhongwen.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Tokenizing with NLTK**\n",
        "- NLTK = [Natural Language ToolKit](https://www.nltk.org/book/)\n",
        "- NLTK has a better method to tokenize words\n",
        "- To use NLTK:\n",
        "    - import the `nltk`` library\n",
        "    - download some resources\n",
        "\n",
        "Then use `nltk.word_tokenize()` to obtain tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import the library and download required resources\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Compare `str.split()` and `nltk.word_tokenize()` on the same text\n",
        "\n",
        "- NLTK function treats punctuation as a token\n",
        "- does this create any new challenges for text metrics?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# .split() keeps punctuation attached to the word\n",
        "split_yadda = yadda.split()\n",
        "\n",
        "print(split_yadda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# nltk tokenizer recognises punctuation\n",
        "nltk_yadda = nltk.word_tokenize(yadda)\n",
        "\n",
        "print(nltk_yadda)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# implications for contractions\n",
        "nltk.word_tokenize('You know we\\'re living in a society!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Tokenize some text!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Word Frequency**\n",
        "\n",
        "- the `nltk.FreqDist()` function quickly provides the frequency distribution of words in your text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create a variable from our turtle text\n",
        "turtles_fdist = nltk.FreqDist(nltk.word_tokenize(turtles))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# look at the whole thing\n",
        "turtles_fdist"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Top 3 most frequent words in this text?\n",
        "turtles_fdist.most_common(3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# frequency of a specific word?\n",
        "turtles_fdist['mutant']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the \".hapaxes\" shows you words with only occur once\n",
        "turtles_fdist.hapaxes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Frequency Distribution Functions\n",
        "\n",
        "`fdist = nltk.FreqDist(tokens)`\n",
        "\n",
        "Information|Syntax\n",
        "-|-\n",
        "Total number of words in the sample|`fdist.N()`\n",
        "Frequency of any particular word|`fdist[word]`\n",
        "Words which occur one time|`fdist.hapaxes()`\n",
        "Top n most frequent words|`fdist.most_common(n)`\n",
        "Plot the distributioon|`fdist.plot()`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### How about a more interesting text?\n",
        "\n",
        "- let's load in a longer text, the short story [They're Made out of Meat](https://www.mit.edu/people/dpolicar/writing/prose/text/thinkingMeat.html)\n",
        "\n",
        "- the cell below first downloads the text from GitHub\n",
        "- then, the Python function `open()` is used to read in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Colab allows for terminal functions like `wget`\n",
        "!wget 'https://raw.githubusercontent.com/scskalicky/VocabAtVic2023NLPWorkshop/main/tmoom.txt'\n",
        "# open the file as a string into Python\n",
        "tmoom = open('tmoom.txt').read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# look at the text\n",
        "tmoom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# maybe we should lowercase the text? easy as.\n",
        "tmoom = tmoom.lower()\n",
        "tmoom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# how many words?\n",
        "len(tmoom.split())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# do you remember why the nltk method results in more 'words'?\n",
        "len(nltk.word_tokenize(tmoom))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# create frequency distribution using nltk tokenization method\n",
        "tmoom_fdist = nltk.FreqDist(nltk.word_tokenize(tmoom))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# what is the frequency of the word \"meat\" in TMOOM?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# what are the five most frequent words in TMOOM?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot the frequency of the top 15 most-frequent words\n",
        "# percents = true for percents rather than raw count\n",
        "tmoom_fdist.plot(15, percents = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### A bigger plot\n",
        "\n",
        "We can expand the code to get a plot of top 50\n",
        "\n",
        "- make the plot bigger\n",
        "- rotate the x axis labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# these first two lines are to make the plot larger\n",
        "import matplotlib.pyplot as plt\n",
        "# resize the plot\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# this creates the plot\n",
        "tmoom_fdist.plot(50, \n",
        "title = 'Top-50 most frequent TMOOM words', \n",
        "percents = True,\n",
        "color = 'blue', \n",
        "#cumulative = True,\n",
        "linestyle = 'dashed', show = False)\n",
        "\n",
        "#this stuff just to rotate the xaxis labels\n",
        "ax = plt.gca()\n",
        "# Rotate the x-axis ticks\n",
        "for tick in ax.get_xticklabels():\n",
        "    tick.set_rotation(70)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Pre-Processing**\n",
        "\n",
        "- Use conditional expressions and tests to remove things we don't want\n",
        "    - remove anything that is punctuation...\n",
        "    - only keep things that are words...\n",
        "    - only keep words of certain lengths...\n",
        "    - remove function/stopwords\n",
        "\n",
        "\n",
        "### Use `if` conditional statements to do this:\n",
        "\n",
        ">```\n",
        ">\n",
        ">if condition is True:\n",
        ">    do something\n",
        ">\n",
        ">```\n",
        "\n",
        "\n",
        "#### Conditional tests include:\n",
        "\n",
        "Test|Syntax\n",
        "--|--\n",
        "is `a` the same as `b`?| `a == b`\n",
        "is `a` greater than `b`?|`a > b`\n",
        "is `a` less than `b`?|`a < b`\n",
        "is `a` in `b?`|`a in b`\n",
        "is `a` not in `b?`|`a not in b`\n",
        "string-specific tests|string-specific syntax\n",
        "is `a` an alphanumeric character?|`a.isalpha()`\n",
        "is `a` uppercased?|`a.isupper()`\n",
        "is `a` lowercased?|`a.islower()`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# is 'a' equal to 'a'?\n",
        "'a' == 'a'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# is 'a' equal to 'b'?\n",
        "'a' == 'b'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### `.isalpha()` is one way to remove punctuation..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'a'.isalpha()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "'!'.isalpha()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Looping though a sequence allows you to perform the test for each item in the sequence:\n",
        "\n",
        "Practice changing the condition to see different results.\n",
        "\n",
        "Try:\n",
        "\n",
        "- `letter.isupper()`\n",
        "- `letter.islower()`\n",
        "- `letter.isalpha()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# sequence through the string\n",
        "for letter in 'New Zealand!':\n",
        "    # apply a test to each item in the string\n",
        "    if ... :\n",
        "        # perform an action is the test is True\n",
        "        print(letter, end = '')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### List Comprehensions\n",
        "\n",
        "- Now that you understand looping and conditionals, time for list comprehensions \n",
        "- List comprehensions quickly create new lists from existing lists (or other sequences/generators)\n",
        "- The general syntax of a list comprehension is as follows:\n",
        "\n",
        "> `[statement/expression for value in sequence/generator]`\n",
        "\n",
        "The list comprehension above uses `[]`, just like a list, and will indeed return a list of values which meet the statement or expression indicated in the *first* part of the list comprehension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ask for each letter in \"word\"\n",
        "# note that the results are a list\n",
        "[letter for letter in 'word']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for each letter in \"New Zealand\", give me the letter if is upper case\n",
        "[letter for letter in \"New Zealand\" if letter.isupper()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for each token in the tokenized list created from \"Hello World!\", give me the token if it is alphanumeric\n",
        "[token for token in nltk.word_tokenize(\"Hello World!\") if token.isalpha()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for each token in the tokenized list created from \"Hello World!\", give me the token if it is alphanumeric\n",
        "# also, lowercase the token\n",
        "[token.lower() for token in nltk.word_tokenize(\"Hello World!\") if token.isalpha()]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, how to obtain a list of all tokens from TMOOM that are *not* punctuation? \n",
        "\n",
        "- Can you complete the list comprehension?\n",
        "- The first ??? should be a call to `nltk.word_tokenize(tmoom)`\n",
        "- The second ??? should be a testing if `word` is alphanumeric. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for each word in the tokenized version of TMOOM, give me the token if it is alphanumeric\n",
        "tmoom_no_punc = [word for word in ??? if ???]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# we can plot this frequency - no more punctuation :) \n",
        "nltk.FreqDist(tmoom_no_punc).plot(10, percents = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Removing stopwords\n",
        "\n",
        "You might also be only interested in removing all of the function words in a text. \n",
        "\n",
        "You can do so by defining a list of words you *don't* want, and asking for a list of all words but those. \n",
        "\n",
        "And, NLTK has a built-in list of stopwords!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.download(\"stopwords\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the stopwords to an list\n",
        "stopwords = nltk.corpus.stopwords.words('english')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# view the stopwords\n",
        "stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# give me the lower cased version of all tokens in the tokenized version of the text\n",
        "tmoom_fdist_no_stopwords = nltk.FreqDist([token.lower() for token in nltk.word_tokenize(tmoom) \n",
        "# if the lowercased version of the text is NOT in stopwords\n",
        "if token.lower() not in stopwords \n",
        "# and if the token is alphanumeric\n",
        "and token.isalpha()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the plot looks much different now!\n",
        "tmoom_fdist_no_stopwords.plot(10, percents = True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Parts of Speech**\n",
        "\n",
        "NLTK can also be used to tag the part of speech for words in your document. \n",
        "\n",
        "\n",
        "We can use the built in NLTK part of speech tagging function.\n",
        "\n",
        "The function expects tokens:\n",
        "\n",
        "> `nltk.pos_tag(tokens)`\n",
        "\n",
        "The results will be a list of `(word,tag)` pairs (which incidently introduces you to another Python data structure, the tuple.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# step 1: have some text\n",
        "gc_quote = \"You know, we're living in a society! We're supposed to act in a civilized way.\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# step 2: tokenize\n",
        "gc_quote_tokens = nltk.word_tokenize(gc_quote)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# step 3: tag\n",
        "gc_quote_pos = nltk.pos_tag(gc_quote_tokens)\n",
        "\n",
        "# look at the resulting (word, tag) pairs\n",
        "[tagged for tagged in gc_quote_pos]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# full list of tags, with definitions and examples\n",
        "nltk.help.upenn_tagset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Frequency + Part of Speech**\n",
        "\n",
        "If you've followed along so far, time for a bit more of an information dump.\n",
        "\n",
        "Let's combine everything so far to find the frequency of various words *and* their part of speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# use the tmoom text\n",
        "tmoom[:25]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# tokenize\n",
        "tmoom_tokens = nltk.word_tokenize(tmoom)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tmoom_pos = nltk.pos_tag(tmoom_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tmoom_pos[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Slicing\n",
        "\n",
        "We need a way to access the first or second thing in these data\n",
        "\n",
        "We can do so with slicing/indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "slicing_example = ['one', 'two', 'three']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the first thing is access with [0]\n",
        "slicing_example[0]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the second with [1]...\n",
        "slicing_example[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# and so on...\n",
        "slicing_example[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The pos tags are pairs like \n",
        "\n",
        "`(word, tag)`\n",
        "\n",
        "So use [0] to get the word, and [1] to get the tag.\n",
        "\n",
        "### Try it out on the TMOOM text.\n",
        "\n",
        "- clean punctuation from the tagged text\n",
        "- create a frequency distribution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# another (better) way to clean punctuation\n",
        "punctuation = \"'?.,[];:'`\"\n",
        "quotes = ['``', \"''\"]\n",
        "\n",
        "# the [0] is a slice which says index the first thing in the list\n",
        "tmoom_clean = [pair for pair in tmoom_pos if pair[0] not in punctuation and pair[0] not in quotes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tmoom_clean[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tmoom_combined_fdist = nltk.FreqDist(tmoom_clean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tmoom_combined_fdist.plot(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tmoom_pos_fdist = nltk.FreqDist([pair[1] for pair in tmoom_clean])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tmoom_pos_fdist.plot(percents = True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyN0SD9Nf71c1FiJtD0Xlkbu",
      "include_colab_link": true,
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
